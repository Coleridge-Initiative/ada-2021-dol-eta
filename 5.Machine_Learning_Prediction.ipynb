{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning Part 2: Prediction Models and Model Evaluation**\n",
    "\n",
    "Tian Lou\n",
    "\n",
    "In this notebook, we use ML models logistic regression, decision tree, and random forest to predict IL certified claimants who will stay in the UI program for 13 weeks or longer. We evaluate each model's performance by looking at accuracy, precision at K, and recall at K. We also compare each model with baselines, i.e., randomly guessing outcomes and guessing everyone will stay for 13 weeks or longer. The data has already been cleaned and transformed to the format we need in the [machine learning data preparation notebook](./4.Machine_Learning_Data_Preparation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Load the Data**\n",
    "\n",
    "As always, let's load R packages and estabilish the database connection first. Note that in this notebook, we include additional packages `caret`, `rpart`, `rpart.plot`, and `randomForest` to get functions for calculating evalution metrics and for fitting decision tree model and random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database interaction imports\n",
    "library(odbc)\n",
    "\n",
    "# For data manipulation/visualization\n",
    "library(tidyverse)\n",
    "\n",
    "# For faster date conversions\n",
    "library(lubridate)\n",
    "\n",
    "# Classification and regression training package. For streamlining the process for creating predictive models\n",
    "library(caret)\n",
    "\n",
    "# For Decision Tree model and plot the tree\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "\n",
    "# For Random Forest model\n",
    "library(randomForest)\n",
    "\n",
    "library(scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "con <- DBI::dbConnect(odbc::odbc(),\n",
    "                     Driver = \"SQL Server\",\n",
    "                     Server = \"msssql01.c7bdq4o2yhxo.us-gov-west-1.rds.amazonaws.com\",\n",
    "                     Database = \"tr_dol_eta\",\n",
    "                     Trusted_Connection = \"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the data and check its top records. <font color=red> Note that you need to change the directory in read.csv() statements below. Replace \". .\" with your username.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data that we have cleaned in the first ML notebook\n",
    "df_ml <- read.csv(\"U:\\\\..\\\\ETA Training\\\\Data\\\\ml_data.csv\")\n",
    "\n",
    "#See the top records of the DataFrame\n",
    "head(df_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the type of each column. Note that the categorical features are still in character type, such as gender and race, or integer type, such as industry code and occupation code. We need to convert them into factor type so that they can be used as dummy variables in ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorical variables into factor type\n",
    "df_ml$gender <- factor(df_ml$gender)\n",
    "df_ml$race <- factor(df_ml$race)\n",
    "df_ml$ethnicity <- factor(df_ml$ethnicity)\n",
    "df_ml$disability <- factor(df_ml$disability)\n",
    "df_ml$education <- factor(df_ml$education)\n",
    "df_ml$naics_maj_code_rv <- factor(df_ml$naics_maj_code_rv)\n",
    "df_ml$occupation_code <- factor(df_ml$occupation_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Split the Training Set and the Testing Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split our data into the training set, `df_training`, and the testing set, `df_testing`. **We will train ML models on cohort 1**, claimants who entered the UI program during the week ending March 28th and the week ending April 4th. **Then, we will validate the models on cohort 2**, claimants who entered the UI program during the week ending June 27th and the week ending July 4th.\n",
    "\n",
    "Moreover, we only need the label and features to run a ML model. Therefore, we remove columns `ssn_id`, `cohort`, and `byr_start_week`, which are person or cohort identifiers and won't be used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training set is the COVID-19 cohort (cohort 1)\n",
    "df_training <- df_ml %>% \n",
    "    filter(cohort == 'cohort1') %>%\n",
    "    select(-c(ssn_id,cohort,byr_start_week)) # remove identifiers, since we do not need them in the ML model\n",
    "\n",
    "# The testing set is the cohort of claimants entered 13 weeks later (cohort 2)\n",
    "df_testing <- df_ml %>% \n",
    "    filter(cohort == 'cohort2') %>%\n",
    "    select(-c(ssn_id,cohort,byr_start_week)) # remove identifiers, since we do not need them in the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Create Functions**\n",
    "\n",
    "Some code will be used many times in this notebook, such as the code to calculate precision and recall, the code to get precision at K, and the code to compare a model's results with baselines. We can put these code in functions so that we can easily repeat these calculation processes by using one line of code.\n",
    "\n",
    "The way to use the functions we create by ourselves is the same as how we use the functions that are already in R. We need to call the function and define the function's arguments. Then the function will return a result, such as a number or a DataFrame. Below is the list of functions we create in this notebooke. We also include instructions of how to use each of the functions and what kinds of results they return. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `precision_recall(test_data, label, pscore)`: calculates precision and recall \n",
    "\n",
    "    After we run a ML model and use its results to predict outcomes for the testing data, we can use the `precision_recall()` function to calculate precision and recall. **This function returns a DataFrame which contains precision and recall at various k (0<k<1).** It has three arguments:\n",
    "    - `test_date`: the DataFrame of the testing data\n",
    "    - `label`: the name of the label column. It should be a **string**.\n",
    "    - `pscore`: the name of the predicted score column. It should be a **string**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `precision_at_k(k, test_data, label, pscore)`: calculate precision at K. \n",
    "\n",
    "    We can use the `precision_at_k()` function to get the precision at the k% we choose. **This function returns a number.** It has four arguments:\n",
    "    - `k`: the percent of population we have enough resources to intervene (e.g., help claimants get back to work)\n",
    "    - `test_date`: the DataFrame of the testing data\n",
    "    - `label`: the name of the label column. It should be a **string**.\n",
    "    - `pscore`: the name of the predicted score column. It should be a **string**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `compare_w_baseline(k, model, test_data, label, pscore)`: compare a model's precision at k with baselines\n",
    "\n",
    "    This function compares our ML model with two baseline models. In the first baseline models, we randomly assign labels (0 or 1) to each person. In the second baseline model, we assign 1 to every person. **This function returns a DataFrame which includes precision at k (we choose the k) of our ML model and the two baseline models.** It has five arguments: \n",
    "    - `k`: the percent of population we have enough resources to intervene (e.g., help claimants get back to work)\n",
    "    - `model`: the name of the ML model. It should be a **string**\n",
    "    - `test_date`: the DataFrame of the testing data\n",
    "    - `label`: the name of the label column. It should be a **string**.\n",
    "    - `pscore`: the name of the predicted score column. It should be a **string**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to run the next three blocks of code so that they will be ready for us to use in the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function, which returns a DataFrame containing precision and recall at K% of the population\n",
    "precision_recall <- function(test_data, label, pscore) {\n",
    "    \n",
    "    # Get the actual label and predicted score in the testing data\n",
    "    df_temp <- df_testing[, c(label, pscore)] \n",
    "    \n",
    "    #Calculate Precision and Recall at K\n",
    "    df_temp <- df_temp %>%\n",
    "        arrange(desc(!!sym(pscore))) %>% # Sort the rows descendingly based on the predicted score\n",
    "        mutate(rank = row_number()) %>% # Add rank to each row\n",
    "        mutate(recall = cumsum(label == 1)/sum(label == 1),  # Calculate Recall at K\n",
    "               precision = cumsum(label == 1)/rank, # Calculate precision at k\n",
    "               k = rank/(nrow(test_data))) # Percent of population\n",
    "    \n",
    "    return(df_temp)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function, which returns precision at K\n",
    "precision_at_k <- function(k=0.1, test_data, label, pscore) {\n",
    "    \n",
    "    # Get the Precision-Recall at K DataFrame\n",
    "    df <- precision_recall(test_data, label, pscore)\n",
    "    \n",
    "    # Assign a few parameters\n",
    "    pct_pop <- k  # Percent of population the resource can cover\n",
    "    test_pop <- nrow(df_testing) # Total number of people in the testing data\n",
    "    pop_at_k <- as.integer(pct_pop * test_pop) # At K percent of the population, how many people the recourse can cover\n",
    "    \n",
    "    # Get precision at K% from the Precision-Recall DataFrame\n",
    "    prec_at_k <- df$precision[pop_at_k]\n",
    "    \n",
    "    return(prec_at_k)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function, which returns a DataFrame containing measures for baseline models \n",
    "# Baseline model 1: Randomly assign the label (0 or 1)\n",
    "# Baseline model 2: Guess everyone is a slow exiter\n",
    "\n",
    "compare_w_baseline <- function(k=0.1, model, test_data, label, pscore) {\n",
    "    # Set a seed so we get consistent results\n",
    "    set.seed(42)\n",
    "    \n",
    "    # Assign a few parameters\n",
    "    pct_pop <- k  # Percent of population the resource can cover\n",
    "    test_pop <- nrow(df_testing) # Total number of people in the testing data\n",
    "    pop_at_k <- as.integer(pct_pop * test_pop) # At K percent of the population, how many people the recourse can cover\n",
    "    \n",
    "    # Get the Precision-Recall at K DataFrame\n",
    "    df <- precision_recall(test_data, label, pscore)\n",
    "    \n",
    "    # Generate Precision-Recall at K for baseline model 1\n",
    "    df_random <- df %>%\n",
    "        mutate(random_score = runif(nrow(df))) %>% # Generate a row of random scores\n",
    "        arrange(desc(random_score)) %>% # Sort the data by the random scores\n",
    "        mutate(random_rank = row_number()) %>% # Add rank to each row\n",
    "        mutate(random_recall = cumsum(label==1)/sum(label==1), # Calculate Recall at K\n",
    "           random_precision = cumsum(label==1)/random_rank, # Calculate Precision at K\n",
    "           random_k = random_rank/(nrow(df)))\n",
    "    \n",
    "    # Precision at K of the model\n",
    "    model_precision_at_k <- precision_at_k(k, test_data, label, pscore)\n",
    "    \n",
    "    # Precision at K of baseline model 1\n",
    "    random_precision_at_k <- df_random$random_precision[pop_at_k]\n",
    "    \n",
    "    # Precision at K of baseline model 2\n",
    "    allstay_precision_at_k <- sum(test_data$label)/test_pop\n",
    "    \n",
    "    # Create a DataFrame which shows all measures\n",
    "    df_compare_prec <- data.frame(\"model\" = c(model, \"Random\", \"All Slow Exiters\"),\n",
    "                              \"precision\" = c(model_precision_at_k, random_precision_at_k, allstay_precision_at_k))\n",
    "    \n",
    "    return(df_compare_prec)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Logistic Regression**\n",
    "\n",
    "In R, we can use the function `glm()` to run a logistic regression model. The first argument, `label ~ .`, means that we want to use the column `label` as the outcome variable and use the rest of the columns as predictors. If you only want to use a few columns as predictors, such as gender and race, you need to specify these columns after `~`, i.e., `label ~ gender + race`. The second argument, `family = binomial (link = 'logit')`, defines the functional form and the error distrition. In the last argument, `data = df_training`, we indicate that we want to train the model on `df_training`. The results are saved in the object `lr_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the logit regression with the training dataset\n",
    "lr_model <- glm(label ~ ., family = binomial(link = 'logit'), data = df_training)\n",
    "\n",
    "#Show feature importance\n",
    "summary(lr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above results, the column **Estimate** shows the importance of each feature in predicting whether a claimant will stay for 13 weeks or longer. The stars at the end of each row indicates whether a feature coefficient is significant. Note that our model automatically leaves out one category for each group of dummy variables. For example, of all the gender dummies (female, male, and unknown), female is the omitted category. \n",
    "\n",
    "We can see that based on our logistic regression model, REDACTED,REDACTED, REDACTED, REDACTED, REDACTED, and REDACTED are important predictors. For example, compared to REDACTED, REDACTED, REDACTED, or even REDACTED are less likely to stay in the UI program for 13 weeks or longer. For another example, claimants from REDACTED (NAICS=REDACTED) and REDACTED (NAICS=REDACTED) indusries are more likely to leave the UI program early. REDACTED claimants and claimants with REDACTED are more likely to stay in the UI program longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Evaluation**\n",
    "\n",
    "Next, let's evaluate the logistic regression's performace. We will start with the confustion matrix and accuracy. Then we look at its precision at 10% of the population and compare it with the two baseline models. Finally, we create the precision and recall curve to investigate how our model perform at different thresholds and different selections of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Accuracy**\n",
    "\n",
    "First, let's use the function `predict()` to predict outcomes of claimants in our testing data. Inside of this function, `lr_model` is the object in which we save the logistic regression's results. `df_testing` is the testing data. `type = 'response` indicates that we want the predicted probabilities (or predicted scores). We add these predicted scores as a column `predict_score` to the DataFrame `df_testing`.\n",
    "\n",
    "Recall that the predicted scores don't tell us the predicted outcome of a claimant. We can say that a claimant with score 0.9 is predicted to be less likely to leave the UI program than a claimant with score 0.7. But whether 0.9 implies the claimant will leave the program or not depends on our choice of the threshold. For example, if we choose a threshold of 0.8, any claimants with predicted scores greater than 0.8 will be defined as slow exiters and any claimants with predicted scores less than 0.8 will be defined as faster exiters. We save the predicted outcomes in a column `predict_label` in the DataFrame `df_testing`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the `confusionMatrix()` function to get the confusion matrix. This function also returns various measures, such as accuracy, sensitivity (or recall), and specificity. You can adjust the threshold based on a specific measure. For example, if you care about how accurate the predicted outcomes are, you can choose a threshold that leads to relatively high accuracy. With our current choice, threshold = 0.8, accuracy is REDACTED, which means REDACTED% of our predicted outcomes are the same as the actual outcomes. When we change the threshold to a higher value, accuracy increases. \n",
    "\n",
    "$$Accuracy = \\frac{True Positive + True Negative}{True Positive + False Positive + True Negative + False Negative}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the slow exiters with the coefficients of our model and the testing set\n",
    "df_testing$predict_score <- predict(lr_model, df_testing, type = 'response')\n",
    "\n",
    "# Set a threshold for the predicted score\n",
    "# Assume people who get more than 0.8 predicted score will be slow exiter\n",
    "threshold <- 0.8\n",
    "\n",
    "# Add the predicted outcome to a new column in df_testing\n",
    "# If predicted score is greater than the threshold, then predict label = 1, otherwise, predict label =0.\n",
    "df_testing$predict_label <- ifelse(df_testing$predict_score > threshold, 1, 0) \n",
    "\n",
    "#Confusion matrix\n",
    "confusionMatrix(factor(df_testing$predict_label), factor(df_testing$label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Precision at K**\n",
    "\n",
    "Most of the time, we may not care about how accurate our model is in predicting both positive (slow exiter) and negative (faster exiter) outcomes. Instead, we may want to check how accurate our model is in predicting positive outcomes, which is captured by **precision**.\n",
    "\n",
    "$$Precision = \\frac{True Positive}{True Positive + False Positive}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we may not choose the threshold directly. Instead, we may want to think about that given our resources, such as funding, time, and staff, what percentage of the population we can help. In the context of this project, we need to decide what percentage of claimants we can provide assistance to so that they can return to work faster. Suppose that we have enough resources to cover 10% of the claimants. We can use the function we created in section 2, `precision_at_k()`, to get the precision at 10% of the population. The function sorts the data descendingly based on the predicted score. The top 10% of claimants with the highest predicted score are predicted to be slow exiter (predicted_label = 1) and the rest are predicted to be fast exiters (predicted_label = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check precision at K%\n",
    "k_pct <- 0.1 # Here, we are checking precision at 10% of the population, change the value to check precision at different K\n",
    "\n",
    "lr_prec_at_10 <- precision_at_k(k_pct, df_testing, \"label\", \"predict_score\")\n",
    "\n",
    "print(paste0(\"In the Logistic Regression Model, precision at \", label_percent()(k_pct), \n",
    "             \" of the population is: \", round(lr_prec_at_10,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result implies that at 10% of the population, among all the claimants the logistic regression model predicts to stay in the UI program for at least 13 weeks, REDACTED% actually stayed for 13 weeks or longer. <font color=red> You can change the value assigned to `k_pct` in the first line of the code to explore precision at other percent of the population.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Compare our model with baselines**\n",
    "\n",
    "How good is the logistic regression's precision at 10% of the population? Is it accurate enough or not? To answer this question, we can compare it with two baseline models. In the first baseline model, we randomly assign label (0 or 1) to each claimant in the testing data. In the second baseline model, we predict that every claimant in the testing data will not leave the UI program before the 13th week. \n",
    "\n",
    "The function we created in section 2, `compare_w_baseline()`, returns a DataFrame which shows percision at k percent of the population for the ML model we trained and the two baseline models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare precision at K% of the population with baseline models\n",
    "df_compare <- compare_w_baseline(k=0.1, \"Logistic Regression\", df_testing, \"label\", \"predict_score\")\n",
    "\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the logistic regression model's precision is lower than the two baseline models' precision. This implies that at 10% of the population, compared to the logistic regression model, we can predict slow exiters more accurately by randomly guessing who will be slow exiters or by guessing that everyone will be slower exiters. \n",
    "\n",
    "The code below creates a bar plot to show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a bar plot to show the comparision of the Logistic Regression model and the baselines (random guess or guess everyone stay)\n",
    "\n",
    "# For easier reading, increase base font size\n",
    "theme_set(theme_gray(base_size = 16))\n",
    "# Adjust repr.plot.width and repr.plot.height to change the size of graphs\n",
    "options(repr.plot.width = 10, repr.plot.height = 5)\n",
    "\n",
    "#Specify source dataset and x and y variables\n",
    "lr_baseline_plot <- ggplot(df_compare, aes(x = model, y = precision)) + \n",
    "    geom_col() + #Plots bars on the graph\n",
    "    geom_text(size = 5, aes(label = format(precision, digit = 3), vjust = -0.5)) + # Show values on top of the bar\n",
    "    scale_y_continuous(breaks = seq(0, 1, 0.2), limits = c(0, 1)) + #Adjust the y scale to set the interval for tick marks\n",
    "    labs(title = \"Precision at 10% against the baseline, Logistic Regression\", # Add graph title\n",
    "         x = \" \", y = 'Precision at 10%') + \n",
    "    theme(axis.title.x = element_text(face=\"bold\"), #Adjust the style of X-axis label\n",
    "          axis.title.y = element_text(face=\"bold\"), #Adjust the styles of the two Y-axes labels\n",
    "          axis.text.x = element_text(face = \"bold\", size = 16),\n",
    "          plot.title = element_text(hjust = 0.5))  #Center the graph title\n",
    "\n",
    "print(lr_baseline_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Precision-Recall Curve**\n",
    "\n",
    "Another measure we often use to evaluate the performance of a ML model is recall (or sensitivity). It shows us what percentage of people with actual positive outcomes our model can capture. In the context of this project, recall tells us what percentage of slow exiters our ML model can accurately predict. \n",
    "\n",
    "In this section, instead of choosing a specific k (percent of population), we use the function we created in section 2, `precision_recall()`, to get precision and recall at various k (0 < k < 1). Then we plot their values on a line chart so that it is easier for us to see how precision and recall change with our choice of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Precision = \\frac{True Positive}{True Positive + False Positive}$$\n",
    "\n",
    "$$Recall = \\frac{True Positive}{True Positive + False Negative}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Precision-Recall at K DataFrame\n",
    "df_measure_at_k <- precision_recall(df_testing, \"label\", \"predict_score\")\n",
    "\n",
    "# See the top records of the DataFrame\n",
    "head(df_measure_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easier reading, increase base font size\n",
    "theme_set(theme_gray(base_size = 16))\n",
    "# Adjust repr.plot.width and repr.plot.height to change the size of graphs\n",
    "options(repr.plot.width = 10, repr.plot.height = 5)\n",
    "\n",
    "#Create the precision-recall curve\n",
    "lr_pr_curve <- ggplot(df_measure_at_k, aes(x=k)) + # Plot percent of population (k) on the x-axis\n",
    "    geom_line(aes(y=precision), color = 'blue') + # Add the precision curve\n",
    "    geom_line(aes(y=recall), color = 'red') + # Add the recall curve\n",
    "    scale_y_continuous(       # We need to create a dual-axis graph, so we need to define two y axes\n",
    "        name = \"Precision\",     # Label of the first axis\n",
    "        sec.axis = sec_axis(~.*1,name=\"Recall\"), # Add a second axis and specify its label\n",
    "        breaks = seq(0, 1, 0.1)) +  # Adjust the tick mark on Y-axis\n",
    "    scale_x_continuous(breaks = seq(0, 1, 0.2)) + # Adjust the tick mark on X-axis\n",
    "    labs(title = \"Precision-Recall Curve, Logistic Regression\", # Add graph title\n",
    "         x = \"Percent of Population\") + # Add X-axis label\n",
    "    theme(axis.title.x = element_text(face=\"bold\"), #Adjust the style of X-axis label\n",
    "          axis.title.y.left = element_text(face=\"bold\", color=\"blue\"), #Adjust the styles of the two Y-axes labels\n",
    "          axis.title.y.right = element_text(face = 'bold', color = 'red'),\n",
    "          plot.title = element_text(hjust = 0.5))  #Center the graph title\n",
    "\n",
    "#Display the graph that we just created\n",
    "print(lr_pr_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, the blue line represents precision. We can see that it stays relatively constant when we increase the choice of k. The red line shows recall. Its values increases as we increase the choice of k. At the 10% of the population, both precision and recall are low. \n",
    "\n",
    "This graph is somewhat different from what we have seen in the class. The precision curve should start at a high point, because when k is low, only claimants with the highest predicted scores will be defined as slow exiters. When we select a higher percent of population, we also relax the threshold and predict claimants with relatively low predicted score to be slow exiters. Therefore, the precision should decrease. \n",
    "\n",
    "This implies that we may either need to choose another model or need to investigate our data. For example, our current features may not be enough to predict claimants' exit decisions. We can add other data sources and create more features. Also, we may need to think about whether the training data we use is representative enough to train a ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Decision Tree**\n",
    "\n",
    "In this section, we run another ML model, decision tree. The function we use is `rpart()`. Similar as before, the first argument, `label ~ . `, defines the formula of the model. The second argument, `method = 'class'` means that we want to use the classification tree method, because our label is binary. In the last argument, we indicate that we want to train the model on data in `df_training`. We save the results in the object `dt_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the decision tree model\n",
    "dt_model <- rpart(label ~ ., method = 'class', data = df_training)\n",
    "\n",
    "# Print results\n",
    "printcp(dt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above result, we can see that the variables the decision tree model actually used to predict outcomes are average total pay, industry, occuaption, and 2019 earnings. This is somewhat consistent with what we have seen in the logistic regression model. \n",
    "\n",
    "Next, we use the function `prp()` to print out the tree and see how the model decides whether a claimant will become a slow exiter. You can adjust the parameters of the `prp()` function to change the display of the tree. `type` decides the overall style of the tree graph. `extra` determines the information shown at each node. Here, we choose to show percentage of observations. We also use `main` to add a title to this tree graph.[<sup>1</sup>](#fn1) <a id = \"13\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tree\n",
    "prp(dt_model, # Your decision tree model\n",
    "    type = 0, # type of trees\n",
    "    extra = 100, # what information to show in each node\n",
    "    main = \"Decision Tree Model\") # Add a title to your decision tree graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the decision tree model perform in predicting slow exiters? Next, let's use the model to predict scores for claimants in the testing data and save the predicted scores in column `df_predict_score`. Then, we calculate the precision and recall and create the precision-recall curve for the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Decision Tree model predicted score and save it in a column in the testing DataFrame\n",
    "df_testing$dt_predict_score <- predict(dt_model, df_testing, type = 'prob')[,2]\n",
    "\n",
    "# Get the Decision Tree model Precision-Recall at K% DataFrame\n",
    "df_dt_measure_at_k <- precision_recall(df_testing, \"label\", \"dt_predict_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easier reading, increase base font size\n",
    "theme_set(theme_gray(base_size = 16))\n",
    "# Adjust repr.plot.width and repr.plot.height to change the size of graphs\n",
    "options(repr.plot.width = 10, repr.plot.height = 5)\n",
    "\n",
    "#Create the precision-recall curve\n",
    "dt_pr_curve <- ggplot(df_dt_measure_at_k, aes(x=k)) + \n",
    "    geom_line(aes(y=precision), color = 'blue') + # Add the precision curve\n",
    "    geom_line(aes(y=recall), color = 'red') + # Add the recall curve\n",
    "    scale_y_continuous(       # We need to create a dual-axis graph, so we need to define two y axes\n",
    "        name = \"Precision\",     # Label of the first axis\n",
    "        sec.axis = sec_axis(~.*1,name=\"Recall\"), # Add a second axis and specify its label\n",
    "        breaks = seq(0, 1, 0.1)) +  # Adjust the tick mark on Y-axis\n",
    "    scale_x_continuous(breaks = seq(0, 1, 0.2)) + # Adjust the tick mark on X-axis\n",
    "    labs(title = \"Precision-Recall Curve, Decision Tree\", # Add graph title\n",
    "         x = \"Percent of Population\") + # Add X-axis label\n",
    "    theme(axis.title.x = element_text(face=\"bold\"), #Adjust the style of X-axis label\n",
    "          axis.title.y.left = element_text(face=\"bold\", color=\"blue\"), #Adjust the styles of the two Y-axes labels\n",
    "          axis.title.y.right = element_text(face = 'bold', color = 'red'),\n",
    "          plot.title = element_text(hjust = 0.5))  #Center the graph title\n",
    "\n",
    "#Display the graph that we just created\n",
    "print(dt_pr_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the precision-recall curve for the decision tree model looks similar to the curve for the logistic regression model. Again, both precision and recall are low at 10% of the population. The decision tree model is also not very accurate in terms of predicting slow exiters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Compare Multiple Models**\n",
    "\n",
    "In this section, we provide you code to compare evaluation metrics for several ML models all at once. First, we \"refresh\" the training data and the testing data to remove columns we created in the previous two sections, such as the predicted scores. This way, we don't accidentally mix them with the results we get in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training set is the COVID-19 cohort (cohort 1)\n",
    "df_training <- df_ml %>% \n",
    "    filter(cohort == 'cohort1') %>%\n",
    "    select(-c(ssn_id,cohort,byr_start_week)) # remove identifiers, since we do not need them in the ML model\n",
    "\n",
    "# The testing set is the cohort of claimants entered 13 weeks later (cohort 2)\n",
    "df_testing <- df_ml %>% \n",
    "    filter(cohort == 'cohort2') %>%\n",
    "    select(-c(ssn_id,cohort,byr_start_week)) # remove identifiers, since we do not need them in the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a few parameters and the DataFrame we use to save the results. `model_list` is a string list which shows all the models we are going to run. In addition to the logistic regression and the decision tree models, we also include the random forest model. You can expand the list if you want to include other ML models. `k` is the percent of population we are able to intervene with available resources. We also create a DataFrame, `df_compare_models`, to store the model names, accuracy, precision at k, and recall at k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list to include all the models we want to compare\n",
    "#LR: Logistic Regresion\n",
    "#DT: Decision Tree\n",
    "#RF: Random Forest\n",
    "model_list <- c(\"LR\", \"DT\", \"RF\")\n",
    "\n",
    "#Define percent of population the resource can cover\n",
    "k <- 0.1\n",
    "pct_pop <- k  # Percent of population the resource can cover\n",
    "test_pop <- nrow(df_testing) # Total number of people in the testing data\n",
    "pop_at_k <- as.integer(pct_pop * test_pop) # At K percent of the population, how many people the resourse can cover\n",
    "\n",
    "#Define an empty DataFrame to save our results\n",
    "n <- length(model_list) #Number of rows of the DataFrame\n",
    "df_compare_models <- data.frame(Model = model_list, accuracy = double(n), \n",
    "                                precision_at_k = double(n), recall_at_k = double(n)) # Define the columns of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use a loop to run all three models and calculate evaluation metrics for each model. The only new model we include in this loop is random forest. We use the function `randomForest` to run the model. Again, we need to define the formula, the data we train the model on, and the method (classification). `ntree` determines the number of trees to grow. `mtry` is the number of variables randomly sampled as candidates at each split. `importance = TRUE` means we want the importance of predictors to be assessed.[<sup>2</sup>](#fn1) <a id = \"14\"> </a>\n",
    "\n",
    "If you would like to include other models, add new `if` statements inside of the loop. Model results should be saved in the object `fit` and predicted scores should be saved in the column `predict_score` of the DataFrame `df_testing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (model in model_list) {\n",
    "    \n",
    "    # Logististic Regression Model\n",
    "    if (model==\"LR\") {\n",
    "        fit <- glm(label ~ ., family = binomial(link = 'logit'), data = df_training) # Fit the model\n",
    "        df_testing$predict_score <- predict(fit, df_testing, type = 'response') # Predict scores\n",
    "    }\n",
    "    \n",
    "    #Decision Tree Model\n",
    "    if (model==\"DT\") {\n",
    "        fit <- rpart(label ~ ., method = 'class', data = df_training) # Fit the model\n",
    "        df_testing$predict_score <- predict(fit, df_testing, type = 'prob')[,2] # Predict scores\n",
    "    }\n",
    "    \n",
    "    #Random Forest Model\n",
    "    if (model == \"RF\"){\n",
    "        fit <- randomForest(label ~ ., data = df_training, type = 'class', ntree = 500, mtry = 6, importance = TRUE) # Fit the model\n",
    "        df_testing$predict_score <- predict(fit, df_testing) # Predict scores\n",
    "    }\n",
    "    \n",
    "    # Get Precision-Recall DataFrame\n",
    "    df_prec_rec <- precision_recall(df_testing, \"label\", \"predict_score\")\n",
    "    \n",
    "    #Calculate accuracy\n",
    "    threshold <- df_prec_rec$predict_score[pop_at_k] # Get the predicted score at K%\n",
    "    df_testing$predict_label <- ifelse(df_testing$predict_score > threshold, 1, 0) # Predict the label, if > threshold, then 1; if < threshold, then 0\n",
    "    df_testing <- df_testing %>% mutate(accurate = 1*(label == predict_label)) # Generate an indicate of whether the prediction is correct\n",
    "    acc = (sum(df_testing$accurate)/nrow(df_testing)) # Calculate accuracy\n",
    "    df_compare_models$accuracy <- ifelse(df_compare_models$Model == model,acc,df_compare_models$accuracy) # Save accuracy to the DataFrame\n",
    "    \n",
    "    #Calculate precision and save it in the df_compare_models DataFrame\n",
    "    prec_at_k <- precision_at_k(k, df_testing, \"label\", \"predict_score\")\n",
    "    df_compare_models$precision_at_k <- ifelse(df_compare_models$Model == model, prec_at_k, df_compare_models$precision_at_k)\n",
    "    \n",
    "    #Calculate Recall and save it in the df_compare_models DataFrame\n",
    "    rec_at_k <- df_prec_rec$recall[pop_at_k]\n",
    "    df_compare_models$recall_at_k <- ifelse(df_compare_models$Model == model, rec_at_k, df_compare_models$recall_at_k)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we run the code in the above cell, we can check the results by looking at `df_compare_model`. We can see that at the 10% of the population, the REDACTED model is the most accurate in predicting both slow exiters and faster exiters (i.e., has the highest accuracy). However, the REDACTED is better at predicting slow exiters and can capture more slow exiters than the other two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Footnotes:**\n",
    "<span id=\"fn1\"> 1. For more information, see <a href='http://www.milbo.org/rpart-plot/prp.pdf'>plotting rpart trees with the rpart.plot package</a>. </span>  \n",
    "\n",
    "[[Go back]](#13)\n",
    "\n",
    "<span id=\"fn2\"> 2. For more information, see <a href='https://cran.r-project.org/web/packages/randomForest/randomForest.pdf'>Breiman and Cutler's Random Forests for Classification and\n",
    "Regression</a>. </span>  \n",
    "\n",
    "[[Go back]](#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
