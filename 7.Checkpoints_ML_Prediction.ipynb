{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Checkpoints**\n",
    "## **Part 2: Prediction Models and Model Evaluation**\n",
    "\n",
    "In this notebook, we provide you with checkpoints to practice on. The structure of this notebook is the same as the second ML notebook's. We removed the texts between code cells so that you can focus on the code. **For detailed explanations about the analysis steps and code, refer back to the [machine learning prediction notebook](./4.Machine_Learning_Prediction.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Load the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database interaction imports\n",
    "library(odbc)\n",
    "\n",
    "# For data manipulation/visualization\n",
    "library(tidyverse)\n",
    "\n",
    "# For faster date conversions\n",
    "library(lubridate)\n",
    "\n",
    "# Classification and regression training package. For streamlining the process for creating predictive models\n",
    "library(caret)\n",
    "\n",
    "# For Decision Tree model and plot the tree\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "\n",
    "# For Random Forest model\n",
    "library(randomForest)\n",
    "\n",
    "library(scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "con <- DBI::dbConnect(odbc::odbc(),\n",
    "                     Driver = \"SQL Server\",\n",
    "                     Server = \"msssql01.c7bdq4o2yhxo.us-gov-west-1.rds.amazonaws.com\",\n",
    "                     Database = \"tr_dol_eta\",\n",
    "                     Trusted_Connection = \"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the data and check its top records. <font color=red> Note that you need to change the directory in read.csv() statements below. Replace \". .\" with your username.</font>\n",
    "\n",
    "> Note that the csv file name is `reg_ml_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data that we have cleaned in the first ML notebook\n",
    "df_ml <- read.csv(\"U:\\\\..\\\\ETA Training\\\\Data\\\\reg_ml_data.csv\")\n",
    "\n",
    "#See the top records of the DataFrame\n",
    "head(df_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorical variables into factor type\n",
    "df_ml$gender <- factor(df_ml$gender)\n",
    "df_ml$race <- factor(df_ml$race)\n",
    "df_ml$ethnicity <- factor(df_ml$ethnicity)\n",
    "df_ml$disability <- factor(df_ml$disability)\n",
    "df_ml$education <- factor(df_ml$education)\n",
    "df_ml$naics_maj_code_rv <- factor(df_ml$naics_maj_code_rv)\n",
    "df_ml$occupation_code <- factor(df_ml$occupation_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checkpoint 1: Split the Training Set and the Testing Set**\n",
    "\n",
    "In this project, we will train ML models on cohort 1, claimants who entered the UI program during the week ending March 28th and the week ending April 4th. Then, we will validate the models on cohort 2, claimants who entered the UI program during the week ending June 27th and the week ending July 4th. Based on this information, split the training set and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training set is the COVID-19 cohort (cohort 1)\n",
    "# Replace '___' with the condition you can use to limit the data to the training set\n",
    "df_training <- df_ml %>% \n",
    "    filter(___) %>%\n",
    "    select(-c(ssn_id,cohort,byr_start_week)) # remove identifiers, since we do not need them in the ML model\n",
    "\n",
    "# The testing set is the cohort of claimants entered 13 weeks later (cohort 2)\n",
    "# Replace '___' with the condition you can use to limit the data to the testing set\n",
    "df_testing <- df_ml %>% \n",
    "    filter(___) %>%\n",
    "    select(-c(ssn_id,cohort,byr_start_week)) # remove identifiers, since we do not need them in the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Create Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just need to run the next three blocks of code so that they will be ready for you to use in the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function, which returns a DataFrame containing precision and recall at K% of the population\n",
    "precision_recall <- function(test_data, label, pscore) {\n",
    "    \n",
    "    # Get the actual label and predicted score in the testing data\n",
    "    df_temp <- df_testing[, c(label, pscore)] \n",
    "    \n",
    "    #Calculate Precision and Recall at K\n",
    "    df_temp <- df_temp %>%\n",
    "        arrange(desc(!!sym(pscore))) %>% # Sort the rows descendingly based on the predicted score\n",
    "        mutate(rank = row_number()) %>% # Add rank to each row\n",
    "        mutate(recall = cumsum(label == 1)/sum(label == 1),  # Calculate Recall at K\n",
    "               precision = cumsum(label == 1)/rank, # Calculate precision at k\n",
    "               k = rank/(nrow(test_data))) # Percent of population\n",
    "    \n",
    "    return(df_temp)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function, which returns precision at K\n",
    "precision_at_k <- function(k=0.1, test_data, label, pscore) {\n",
    "    \n",
    "    # Get the Precision-Recall at K DataFrame\n",
    "    df <- precision_recall(test_data, label, pscore)\n",
    "    \n",
    "    # Assign a few parameters\n",
    "    pct_pop <- k  # Percent of population the resource can cover\n",
    "    test_pop <- nrow(df_testing) # Total number of people in the testing data\n",
    "    pop_at_k <- as.integer(pct_pop * test_pop) # At K percent of the population, how many people the recourse can cover\n",
    "    \n",
    "    # Get precision at K% from the Precision-Recall DataFrame\n",
    "    prec_at_k <- df$precision[pop_at_k]\n",
    "    \n",
    "    return(prec_at_k)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function, which returns a DataFrame containing measures for baseline models \n",
    "# Baseline model 1: Randomly assign the label (0 or 1)\n",
    "# Baseline model 2: Guess everyone is a slow exiter\n",
    "\n",
    "compare_w_baseline <- function(k=0.1, model, test_data, label, pscore) {\n",
    "    # Set a seed so we get consistent results\n",
    "    set.seed(42)\n",
    "    \n",
    "    # Assign a few parameters\n",
    "    pct_pop <- k  # Percent of population the resource can cover\n",
    "    test_pop <- nrow(df_testing) # Total number of people in the testing data\n",
    "    pop_at_k <- as.integer(pct_pop * test_pop) # At K percent of the population, how many people the recourse can cover\n",
    "    \n",
    "    # Get the Precision-Recall at K DataFrame\n",
    "    df <- precision_recall(test_data, label, pscore)\n",
    "    \n",
    "    # Generate Precision-Recall at K for baseline model 1\n",
    "    df_random <- df %>%\n",
    "        mutate(random_score = runif(nrow(df))) %>% # Generate a row of random scores\n",
    "        arrange(desc(random_score)) %>% # Sort the data by the random scores\n",
    "        mutate(random_rank = row_number()) %>% # Add rank to each row\n",
    "        mutate(random_recall = cumsum(label==1)/sum(label==1), # Calculate Recall at K\n",
    "           random_precision = cumsum(label==1)/random_rank, # Calculate Precision at K\n",
    "           random_k = random_rank/(nrow(df)))\n",
    "    \n",
    "    # Precision at K of the model\n",
    "    model_precision_at_k <- precision_at_k(k, test_data, label, pscore)\n",
    "    \n",
    "    # Precision at K of baseline model 1\n",
    "    random_precision_at_k <- df_random$random_precision[pop_at_k]\n",
    "    \n",
    "    # Precision at K of baseline model 2\n",
    "    allstay_precision_at_k <- sum(test_data$label)/test_pop\n",
    "    \n",
    "    # Create a DataFrame which shows all measures\n",
    "    df_compare_prec <- data.frame(\"model\" = c(model, \"Random\", \"All Slow Exiters\"),\n",
    "                              \"precision\" = c(model_precision_at_k, random_precision_at_k, allstay_precision_at_k))\n",
    "    \n",
    "    return(df_compare_prec)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the logit regression with the training dataset\n",
    "lr_model <- glm(label ~ ., family = binomial(link = 'logit'), data = df_training)\n",
    "\n",
    "#Show feature importance\n",
    "summary(lr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checkpoint 2: Evaluate the Logitist Regression**\n",
    "\n",
    "1. In the confustion matrix section, change the value of the threshold to see how the logistic regression's accuracy change. Which threshold will you use eventually and why?\n",
    "\n",
    "2. In the precision at K section, assign a value to `k_pct`. Why do you choose this value? What's the interpretation of the precision you get?\n",
    "\n",
    "3. In the compare our model with baselines section, assign a value to k. How well does your model perform compared to the baseline models?\n",
    "\n",
    "4. Comment on the precision-recall curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the slow exiters with the coefficients of our model and the testing set\n",
    "df_testing$predict_score <- predict(lr_model, df_testing, type = 'response')\n",
    "\n",
    "# Set a threshold for the predicted score\n",
    "# Assume people who get more than ___ predicted score will be slow exiter\n",
    "# Replace '___' with the threshold of your choice\n",
    "threshold <- ___\n",
    "\n",
    "# Add the predicted outcome to a new column in df_testing\n",
    "# If predicted score is greater than the threshold, then predict label = 1, otherwise, predict label =0.\n",
    "df_testing$predict_label <- ifelse(df_testing$predict_score > threshold, 1, 0) \n",
    "\n",
    "#Confusion matrix\n",
    "confusionMatrix(factor(df_testing$predict_label), factor(df_testing$label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Precision at K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check precision at K%\n",
    "# Replace '___' with the value of your choice. Note that the value should be greater than 0 and less than 1.\n",
    "k_pct <- ___ # Here, we are checking precision at ___% of the population, change the value to check precision at different K\n",
    "\n",
    "lr_prec_at_10 <- precision_at_k(k_pct, df_testing, \"label\", \"predict_score\")\n",
    "\n",
    "print(paste0(\"In the Logistic Regression Model, precision at \", label_percent()(k_pct), \n",
    "             \" of the population is: \", round(lr_prec_at_10,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Compare our model with baselines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare precision at K% of the population with baseline models\n",
    "# Replace '___' with a value of your choice. It should be greater than 0 and less than 1.\n",
    "df_compare <- compare_w_baseline(k=___, \"Logistic Regression\", df_testing, \"label\", \"predict_score\")\n",
    "\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a bar plot to show the comparision of the Logistic Regression model and the baselines (random guess or guess everyone stay)\n",
    "# Replace '___' in the labs() layer title with the k of your choice\n",
    "\n",
    "# For easier reading, increase base font size\n",
    "theme_set(theme_gray(base_size = 16))\n",
    "# Adjust repr.plot.width and repr.plot.height to change the size of graphs\n",
    "options(repr.plot.width = 10, repr.plot.height = 5)\n",
    "\n",
    "#Specify source dataset and x and y variables\n",
    "lr_baseline_plot <- ggplot(df_compare, aes(x = model, y = precision)) + \n",
    "    geom_col() + #Plots bars on the graph\n",
    "    geom_text(size = 5, aes(label = format(precision, digit = 3), vjust = -0.5)) + # Show values on top of the bar\n",
    "    scale_y_continuous(breaks = seq(0, 1, 0.2), limits = c(0, 1)) + #Adjust the y scale to set the interval for tick marks\n",
    "    labs(title = \"Precision at ___% against the baseline, Logistic Regression\", # Add graph title\n",
    "         x = \" \", y = 'Precision at __%') + \n",
    "    theme(axis.title.x = element_text(face=\"bold\"), #Adjust the style of X-axis label\n",
    "          axis.title.y = element_text(face=\"bold\"), #Adjust the styles of the two Y-axes labels\n",
    "          axis.text.x = element_text(face = \"bold\", size = 16),\n",
    "          plot.title = element_text(hjust = 0.5))  #Center the graph title\n",
    "\n",
    "print(lr_baseline_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Precision-Recall Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Precision-Recall at K DataFrame\n",
    "df_measure_at_k <- precision_recall(df_testing, \"label\", \"predict_score\")\n",
    "\n",
    "# See the top records of the DataFrame\n",
    "head(df_measure_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easier reading, increase base font size\n",
    "theme_set(theme_gray(base_size = 16))\n",
    "# Adjust repr.plot.width and repr.plot.height to change the size of graphs\n",
    "options(repr.plot.width = 10, repr.plot.height = 5)\n",
    "\n",
    "#Create the precision-recall curve\n",
    "lr_pr_curve <- ggplot(df_measure_at_k, aes(x=k)) + # Plot percent of population (k) on the x-axis\n",
    "    geom_line(aes(y=precision), color = 'blue') + # Add the precision curve\n",
    "    geom_line(aes(y=recall), color = 'red') + # Add the recall curve\n",
    "    scale_y_continuous(       # We need to create a dual-axis graph, so we need to define two y axes\n",
    "        name = \"Precision\",     # Label of the first axis\n",
    "        sec.axis = sec_axis(~.*1,name=\"Recall\"), # Add a second axis and specify its label\n",
    "        breaks = seq(0, 1, 0.1)) +  # Adjust the tick mark on Y-axis\n",
    "    scale_x_continuous(breaks = seq(0, 1, 0.2)) + # Adjust the tick mark on X-axis\n",
    "    labs(title = \"Precision-Recall Curve, Logistic Regression\", # Add graph title\n",
    "         x = \"Percent of Population\") + # Add X-axis label\n",
    "    theme(axis.title.x = element_text(face=\"bold\"), #Adjust the style of X-axis label\n",
    "          axis.title.y.left = element_text(face=\"bold\", color=\"blue\"), #Adjust the styles of the two Y-axes labels\n",
    "          axis.title.y.right = element_text(face = 'bold', color = 'red'),\n",
    "          plot.title = element_text(hjust = 0.5))  #Center the graph title\n",
    "\n",
    "#Display the graph that we just created\n",
    "print(lr_pr_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the decision tree model\n",
    "dt_model <- rpart(label ~ ., method = 'class', data = df_training)\n",
    "\n",
    "# Print results\n",
    "printcp(dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tree\n",
    "prp(dt_model, # Your decision tree model\n",
    "    type = 0, # type of trees\n",
    "    extra = 100, # what information to show in each node\n",
    "    main = \"Decision Tree Model\") # Add a title to your decision tree graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Decision Tree model predicted score and save it in a column in the testing DataFrame\n",
    "df_testing$dt_predict_score <- predict(dt_model, df_testing, type = 'prob')[,2]\n",
    "\n",
    "# Get the Decision Tree model Precision-Recall at K% DataFrame\n",
    "df_dt_measure_at_k <- precision_recall(df_testing, \"label\", \"dt_predict_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easier reading, increase base font size\n",
    "theme_set(theme_gray(base_size = 16))\n",
    "# Adjust repr.plot.width and repr.plot.height to change the size of graphs\n",
    "options(repr.plot.width = 10, repr.plot.height = 5)\n",
    "\n",
    "#Create the precision-recall curve\n",
    "dt_pr_curve <- ggplot(df_dt_measure_at_k, aes(x=k)) + \n",
    "    geom_line(aes(y=precision), color = 'blue') + # Add the precision curve\n",
    "    geom_line(aes(y=recall), color = 'red') + # Add the recall curve\n",
    "    scale_y_continuous(       # We need to create a dual-axis graph, so we need to define two y axes\n",
    "        name = \"Precision\",     # Label of the first axis\n",
    "        sec.axis = sec_axis(~.*1,name=\"Recall\"), # Add a second axis and specify its label\n",
    "        breaks = seq(0, 1, 0.1)) +  # Adjust the tick mark on Y-axis\n",
    "    scale_x_continuous(breaks = seq(0, 1, 0.2)) + # Adjust the tick mark on X-axis\n",
    "    labs(title = \"Precision-Recall Curve, Decision Tree\", # Add graph title\n",
    "         x = \"Percent of Population\") + # Add X-axis label\n",
    "    theme(axis.title.x = element_text(face=\"bold\"), #Adjust the style of X-axis label\n",
    "          axis.title.y.left = element_text(face=\"bold\", color=\"blue\"), #Adjust the styles of the two Y-axes labels\n",
    "          axis.title.y.right = element_text(face = 'bold', color = 'red'),\n",
    "          plot.title = element_text(hjust = 0.5))  #Center the graph title\n",
    "\n",
    "#Display the graph that we just created\n",
    "print(dt_pr_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Compare Multiple Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training set is the COVID-19 cohort (cohort 1)\n",
    "df_training <- df_ml %>% \n",
    "    filter(cohort == 'cohort1') %>%\n",
    "    select(-c(ssn_id,cohort,byr_start_week)) # remove identifiers, since we do not need them in the ML model\n",
    "\n",
    "# The testing set is the cohort of claimants entered 13 weeks later (cohort 2)\n",
    "df_testing <- df_ml %>% \n",
    "    filter(cohort == 'cohort2') %>%\n",
    "    select(-c(ssn_id,cohort,byr_start_week)) # remove identifiers, since we do not need them in the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list to include all the models we want to compare\n",
    "#LR: Logistic Regresion\n",
    "#DT: Decision Tree\n",
    "#RF: Random Forest\n",
    "model_list <- c(\"LR\", \"DT\", \"RF\")\n",
    "\n",
    "#Define percent of population the resource can cover\n",
    "k <- 0.1\n",
    "pct_pop <- k  # Percent of population the resource can cover\n",
    "test_pop <- nrow(df_testing) # Total number of people in the testing data\n",
    "pop_at_k <- as.integer(pct_pop * test_pop) # At K percent of the population, how many people the resourse can cover\n",
    "\n",
    "#Define an empty DataFrame to save our results\n",
    "n <- length(model_list) #Number of rows of the DataFrame\n",
    "df_compare_models <- data.frame(Model = model_list, accuracy = double(n), \n",
    "                                precision_at_k = double(n), recall_at_k = double(n)) # Define the columns of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (model in model_list) {\n",
    "    \n",
    "    # Logististic Regression Model\n",
    "    if (model==\"LR\") {\n",
    "        fit <- glm(label ~ ., family = binomial(link = 'logit'), data = df_training) # Fit the model\n",
    "        df_testing$predict_score <- predict(fit, df_testing, type = 'response') # Predict scores\n",
    "    }\n",
    "    \n",
    "    #Decision Tree Model\n",
    "    if (model==\"DT\") {\n",
    "        fit <- rpart(label ~ ., method = 'class', data = df_training) # Fit the model\n",
    "        df_testing$predict_score <- predict(fit, df_testing, type = 'prob')[,2] # Predict scores\n",
    "    }\n",
    "    \n",
    "    #Random Forest Model\n",
    "    if (model == \"RF\"){\n",
    "        fit <- randomForest(label ~ ., data = df_training, type = 'class', ntree = 500, mtry = 6, importance = TRUE) # Fit the model\n",
    "        df_testing$predict_score <- predict(fit, df_testing) # Predict scores\n",
    "    }\n",
    "    \n",
    "    # Get Precision-Recall DataFrame\n",
    "    df_prec_rec <- precision_recall(df_testing, \"label\", \"predict_score\")\n",
    "    \n",
    "    #Calculate accuracy\n",
    "    threshold <- df_prec_rec$predict_score[pop_at_k] # Get the predicted score at K%\n",
    "    df_testing$predict_label <- ifelse(df_testing$predict_score > threshold, 1, 0) # Predict the label, if > threshold, then 1; if < threshold, then 0\n",
    "    df_testing <- df_testing %>% mutate(accurate = 1*(label == predict_label)) # Generate an indicate of whether the prediction is correct\n",
    "    acc = (sum(df_testing$accurate)/nrow(df_testing)) # Calculate accuracy\n",
    "    df_compare_models$accuracy <- ifelse(df_compare_models$Model == model,acc,df_compare_models$accuracy) # Save accuracy to the DataFrame\n",
    "    \n",
    "    #Calculate precision and save it in the df_compare_models DataFrame\n",
    "    prec_at_k <- precision_at_k(k, df_testing, \"label\", \"predict_score\")\n",
    "    df_compare_models$precision_at_k <- ifelse(df_compare_models$Model == model, prec_at_k, df_compare_models$precision_at_k)\n",
    "    \n",
    "    #Calculate Recall and save it in the df_compare_models DataFrame\n",
    "    rec_at_k <- df_prec_rec$recall[pop_at_k]\n",
    "    df_compare_models$recall_at_k <- ifelse(df_compare_models$Model == model, rec_at_k, df_compare_models$recall_at_k)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checkpoint 3: Compare Multiple Models's Results**\n",
    "\n",
    "Based on the results in the DataFrame, `df_compare_model`, which model would you choose to predict slow exiters in this project? Which measure(s) do you use to select the model and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
