{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"450\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span> \n",
    "\n",
    "# **<center>Machine Learning Part 1: Data Preparation</center>**\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6368905.svg)](https://doi.org/10.5281/zenodo.6368905)\n",
    "\n",
    "<br>\n",
    "<center>Tian Lou</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction**\n",
    "\n",
    "In the [cohort analysis notebook](./2.Data_Exploration_Cohort_Analysis.ipynb), we have tracked records of the COVID-19 cohort (claimants who entered during the week ending March 28th and the week ending April 4th, 2020) and have calculated their exit rates. We have also examined how claimants from different industries exit at various rates. For example, we found that claimants from REDACTED and REDACTED have slower exit rates than claimants from other industries. However, industry is just one of the factors that are related to claimants' exit rates. What about other factors, such as demographics, occupation, prior earnings, location, and total amount of benefits received from the UI program? Can we use these factors to predict which group of workers are the mostly likely to stay in the UI program and eventually exhaust their benefits? Can interventions, such as economic shocks and/or training programs, accelerate claimants' rate of returning to work? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two machine learning (ML) notebooks, we will develop a machine learning model to answer these questions. We will use it to predict claimants who are the most likely to stay in the UI program for 13 weeks or longer and to examine how economic shocks influence claimants' exit rates. In the first ML notebook, we will formulate the research questions and clean up the data, including identifying and constructing rows, and creating labels (outcome, Y) and features (predictors, X). In the second ML notebook, we will split the data into a training set and a testing set. We will train a few models, such as logistic regression, decision tree, and random forest. Finally, we will evaluate these models by using several measures, such as accuracy, precision at K, and recall at K. \n",
    "\n",
    "After you work through the machine learning notebooks, we have two checkpoint notebooks for you to practice the code. You can think about how you might apply any of the techniques and code presented in these notebooks to your project. The model we develop in these notebooks can be used to predict other outcomes, such as claimants who are at risk of exhausting their benefits, and to examine the effectiveness of other interventions, such as different job search assistance services and job training programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Learning Objectives**\n",
    "\n",
    "Throughout this series of notebooks, our overarching questions are: How can states best produce information that can be used by local workforce boards to help inform resource allocation for unemployed workers? What information is the most useful for workforce boards for strategic resource planning and efficient resource allocation? What measures are most useful to local workforce boards and how do we characterize variation in those measures? We use Illinois administrative data, Bureau of Labor Statistics (BLS) workforce data, and Opportunity Insights Economic Tracker data as an example and show you step by step how we develop a project, including: exploring the data set, selecting sample, defining outcome measures, conducting subgroup analyses, creating visualizations, and building a prediction machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you finish the two ML notebooks, you should know:\n",
    "- What a row means and how to create rows by using the certified claims data\n",
    "- How to use consumer spending data and how to join it with the claims data\n",
    "- How to create the label (outcome variable, Y)\n",
    "- How to create features (predictor variables, X)\n",
    "- How to deal with missing values\n",
    "- How to split data into a training set and a testing set\n",
    "- How to run ML models\n",
    "- How to create evaluation measures (e.g., accuracy, precision, recall)\n",
    "- How to evaluate ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Research Questions**\n",
    "\n",
    "In this project, we are interested in **predicting claimants who will stay in the UI program for 13 weeks or longer**. We define claimants who left before the 13th week as **fast exiters** and those who left during or after the 13th week as **slow exiters**. The questions we seek to answer are:\n",
    "\n",
    "- Which claimants will stay in the UI program for 13 weeks or longer?\n",
    "- How important is an economic shock in determining whether a claimant becomes a slow exiter? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Datasets**\n",
    "We will use the Illinois PROMIS file and the Consumer Spending data from the Opportunity Insights Economic Tracker:\n",
    "- **2020 Illinois PROMIS certified claims file**: weekly UI claims data. Each record represents a certified claim in a certain week. The data has a claimant's demographics, education level, prior industry, occupation, and locations. It also contains detailed information about the claim, such as program type, claim type, certification status, benefit starting date, and benefit amount. Federal Pandemic Unemployment Compensation (FPUC, $600/week) and dependent benefits are included in the total amount paid.\n",
    "\n",
    "The full certified claims data has more than REDACTED million rows. Using the full dataset for data exploration is time-consuming. Therefore, **we will use a 1% random sample of the certified claims data in all notebooks. You should also use the 1% random sample when walking through all notebooks and when identifying the scope of your project.** After you decide your research questions and analysis scope, only pull the data and the variables you need from the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Opportunity Insights Economic Tracker, Consumer Spending Data**[<sup>1</sup>](#fn1) <a id = \"9\"> </a>: county-level daily credit/debit card spending relative to January 4-21 2020 in all merchant category codes (MCC). The data is seasonally adjusted and is presented as a 7 day lookback moving average. It captures about 10% of debit and credit card spending in the U.S and covers a broad range of industries, but over-represents spending in industries such as accommodation and food services and clothing. Due to privacy and confidentiality reasons, small cells and outliers have been suppressed. Only REDACTED (out of REDACTED) IL counties' spending data are consistently available over time. [<sup>2</sup>](#fn2) <a id = \"10\"> </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analytical Methods**\n",
    "The specific techniques include but not limited to:\n",
    "- **SQL statements/keywords**:\n",
    " - `SELECT ... FROM`: select data from a table in the database\n",
    " - `WHERE`: select subset of tables from the database\n",
    " - `CONVERT`/`CAST`: convert one data type into another\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **R code**:\n",
    " - `str_pad`: pad a string, such as adding leading zeros\n",
    " - `replace`: replace values in a column\n",
    " - `scale`: standardize/normalize values in a column\n",
    " - `factor`: convert a character/numeric type column to factor type\n",
    " - `glm`: fit generalized linear models. In this project, we use it for logistic regression\n",
    " - `rpart`: fit decision tree model\n",
    " - `randomForest`: fit random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Load the Data**\n",
    "\n",
    "In this notebook, we use data in the table `il_des_promis_1pct` and the table `affinity_county_daily` (consumer spending data) in the schema `tr_dol_eta.dbo`. First, we need to load R packages and estabilish the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database interaction imports\n",
    "library(odbc)\n",
    "\n",
    "# For data manipulation/visualization\n",
    "library(tidyverse)\n",
    "\n",
    "# For faster date conversions\n",
    "library(lubridate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "con <- DBI::dbConnect(odbc::odbc(),\n",
    "                     Driver = \"SQL Server\",\n",
    "                     Server = \"msssql01.c7bdq4o2yhxo.us-gov-west-1.rds.amazonaws.com\",\n",
    "                     Database = \"tr_dol_eta\",\n",
    "                     Trusted_Connection = \"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **IL PROMIS Certified Claims File**\n",
    "\n",
    "**We want to predict claimants who will stay in the UI program for 13 weeks or longer with a machine learning model.** In the two data exploration notebooks, we selected the data by using `week_end_date` to get the cross-sectional sample and sliced the data by using `byr_start_week` to get the cohort sample. Only the latter one allows us to observe a claimant's records from program entry to program exit and allows us to calculate how long a claimant stayed in the UI program. Therefore, to construct the data for the machine learning model, we select the data the same way as what we did in the cohort analysis.  \n",
    "\n",
    "We need at least two cohorts to develop the model. We will train the model on the first cohort and validate it on the second cohort. Therefore, our data includes the COVID-19 cohort (claimants who entered during the week ending March 28th and the week ending April 4th, 2020) and a cohort of claimants who entered 13 weeks later (claimants who entered during the week ending June 27th and the week ending July 4th, 2020). The table below shows the number of weeks after program entry for both cohorts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||Week Ending Date|3-28|4-4|4-11|4-18|...|6-20|6-27|7-4|7-11|7-18|...|9-19|9-26|...|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|**Training set**|**Cohort 1** |1|2| 3 | 4 |...| 13 |...|\n",
    "|**Training set**|**Cohort 1** | | 1 | 2 | 3 |...| 12 | 13 |...|\n",
    "|**Testing set**|**Cohort 2**|||||||1|2|3|4|...|13|...|\n",
    "|**Testing set**|**Cohort 2**||||||||1|2|3|...|12|13|...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQL query we use to load the data is the same as the query we use in the cohort analysis notebook. We still only look at Regular state UI program (`program_type=1 AND sub_program_type=1`) and new claims (`claim_type=1`). The only difference is that we need to select four benefit year starting weeks (`byr_start_week in ('2020-03-28','2020-04-04','2020-06-27','2020-07-04')`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select PROMIS certified claimant records from the database to a dataframe\n",
    "\n",
    "# Store SQL query to a character variable\n",
    "query <- \"\n",
    "SELECT ssn_id,\n",
    "    week_end_date,\n",
    "    byr_start_week,\n",
    "    birth_date,\n",
    "    gender,\n",
    "    race,\n",
    "    ethnicity,\n",
    "    disability,\n",
    "    education,\n",
    "    county_fips_code,\n",
    "    naics_code,\n",
    "    occupation_code,\n",
    "    total_pay,\n",
    "    wages_2019\n",
    "FROM tr_dol_eta.dbo.il_des_promis_1pct\n",
    "WHERE sub_program_type = 1\n",
    "AND program_type = 1\n",
    "AND claim_type = 1\n",
    "AND byr_start_week in ('2020-03-28','2020-04-04','2020-06-27','2020-07-04');\n",
    "\"\n",
    "\n",
    "# Execute query\n",
    "df_claimants <-dbGetQuery(con,query)\n",
    "\n",
    "# R interprets dates as character when pulling from the database, must convert with ymd()\n",
    "df_claimants <- df_claimants %>%\n",
    "    mutate(week_end_date=ymd(week_end_date),\n",
    "          byr_start_week=ymd(byr_start_week),\n",
    "          birth_date=ymd(birth_date))\n",
    "\n",
    "# See top records in the dataframe\n",
    "head(df_claimants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new column `cohort` to `df_claimants`. It indicates which cohort a claimant is from. We will use this cohort indicator frequenly in the later analysis, such as to join the claims data and the consumer spending data and to split the training set and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the cohort indicator to df_claimants\n",
    "df_claimants <- df_claimants %>%\n",
    "    mutate(cohort = case_when(byr_start_week == '2020-03-28'|byr_start_week == '2020-04-04' ~ 'cohort1',\n",
    "                              byr_start_week == '2020-06-27'|byr_start_week == '2020-07-04' ~ 'cohort2',\n",
    "                              TRUE ~ 'other'))\n",
    "\n",
    "#See top records of the cohort indicator we just generated\n",
    "head(df_claimants %>% select(ssn_id, byr_start_week, cohort))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Opportunity Insights Economic Tracker: Consumer Spending Data**\n",
    "\n",
    "In addition to the variables in the PROMIS file, we also include county-level consumer spending as a feature and use it to capture local economic shocks. The data is stored in the table `affinity_county_daily` in the schema `tr_dol_eta.dbo`. In the query below, we combine the `year`, `month`, and `day` columns into one variable `spend_date` by using the `CONVERT()` and `CAST()` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select IL Consumer spending data from the database to a dataframe\n",
    "\n",
    "# Store SQL query to a character variable\n",
    "# In this query, we use CONVERT() and CAST() statements to combine the year, month, and day columns into a date variable 'spend_date'\n",
    "query <- \"\n",
    "SELECT CONVERT(date, CAST([year] AS varchar(4)) + '-' +  \n",
    "                     CAST([month] AS varchar(2)) + '-' + \n",
    "                     CAST([day] AS varchar(2))) AS spend_date,\n",
    "    county_fips,\n",
    "    spend_all\n",
    "FROM tr_dol_eta.dbo.affinity_county_daily;\n",
    "\"\n",
    "\n",
    "# Execute query\n",
    "df_spend <-dbGetQuery(con,query)\n",
    "\n",
    "# R interprets dates as character when pulling from the database, must convert with ymd()\n",
    "df_spend <- df_spend %>%\n",
    "    mutate(spend_date=ymd(spend_date))\n",
    "\n",
    "# See top records in the dataframe\n",
    "head(df_spend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three columns in the DataFrame `df_spend`: 1) `spend_date`, indicates which day the spending data is for; 2) `county_fips`, the last three digits are county FIPS code and the first one or two digits are state FIPS code; 3) `spend_all`, daily credit/debit card spending index of a county. The spending index is seasonally adjusted and is 7-day moving average. \n",
    "\n",
    "> The earlist spending data available to us is from 1/13/2020. That's why `spend_all` is NA in the first few rows.[<sup>3</sup>](#fn3) <a id = \"11\"> </a>\n",
    "\n",
    "The data contains spending indexes for multiple states. We need to limit it to IL data by using FIPS code. First, a few states' FIPS codes are four-digit. We use the `str_pad` function to add a leading zero to these states' FIPS codes so that we can create state FIPS codes correctly. Second, we use the `substr` function to separate `county_fips` into two-digit state FIPS code and three-digit county FIPS code. Finally, we filter the data to `state_fips==REDACTED` (IL) and also remove an IL county that does not show consistently in the data over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up FIPS code and limit the spending data to IL counties that show in the data consistently over time\n",
    "df_spend <- df_spend %>%\n",
    "    mutate(county_fips = str_pad(county_fips, 5, side = c(\"left\"), pad=\"0\")) %>% # Add a leading zero to county fips code\n",
    "    mutate(state_fips = substr(county_fips, 1, 2), county_fips_code = substr(county_fips, 3, 5)) %>% # Generate two-digit state FIPS code and three-digit county FIPS code\n",
    "    filter(state_fips == 'REDACTED' & county_fips_code!= 'REDACTED') %>% # Only keep IL data and remove a county that is not consistenly available during the time period we look at\n",
    "    select(spend_date, spend_all, county_fips_code) # Only keep the columns we need\n",
    "\n",
    "#Check county_fips_code and each FIPS code's frequency\n",
    "table(df_spend$county_fips_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we use the **county-level average consumer spending during one week, four weeks, and eight weeks before each cohort's entry** as features. It captures short-term and long-term local economic activities prior to a claimant's job separation. We will test its importance in determining how long claimants stay in the UI program. One might argue that the economic conditions after UI program entry might also affect a claimantâ€™s exit decision. For example, when the economy is expanding, there are more job opportunities and claimants are more likely to leave the program. However, we cannot use it as a feature in the prediction machine learning model, because when we predict cohort 2's (6/27 and 7/4 entrants) exit decisions, we don't know the future economic conditions. Imagine that you are using this model on 7/5/2020. You don't know how consumer spending will change in the next 13 weeks. **In general, we must avoid using future data to predict outcomes**. \n",
    "\n",
    "The variable we suggest here is not the only way you can use the consumer spending data. Depending on your assumptions about how economic activities may impact UI claimants' exit decisions, you may create the variables in different ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weekly average spending\n",
    "df_weekly_spend <- df_spend %>% \n",
    "    mutate(spend_week = epiweek(spend_date)) %>% # Create a week indicator. For example, any date between 12-29-2019 and 01-04-2020 will be labelled as 1.\n",
    "    group_by(spend_week, county_fips_code) %>% \n",
    "    summarize(avg_spend = mean(spend_all)) # Calculate average weekly spending index for each county\n",
    "\n",
    "#See the top records\n",
    "head(df_weekly_spend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the two cohorts' week indicators\n",
    "cohort1_week <- epiweek('2020-03-28')\n",
    "cohort2_week <- epiweek('2020-06-27')\n",
    "\n",
    "# Average weekly spending indexes one week, four weeks, and eight weeks prior to Cohort 1's entry\n",
    "df_cohort1_avg_spend <- df_weekly_spend %>%\n",
    "    filter(spend_week %in% c(cohort1_week-1, cohort1_week-4, cohort1_week-8)) %>% # Subset the data to 1, 4, 8 weeks prior program entry week\n",
    "    pivot_wider(names_from = spend_week, values_from = avg_spend) %>% # Reshape the data to wide format so that each week's spending index is a column\n",
    "    `colnames<-`(c('county_fips_code', 'avg_spend_8', 'avg_spend_4', 'avg_spend_1')) %>% # Rename columns\n",
    "    mutate(cohort = 'cohort1') # Generate the cohort indicator\n",
    "\n",
    "# Average weekly spending indexes one week, four weeks, and eight weeks prior to Cohort 1's entry\n",
    "df_cohort2_avg_spend <- df_weekly_spend %>%\n",
    "    filter(spend_week %in% c(cohort2_week-1, cohort2_week-4, cohort2_week-8)) %>% # Subset the data to 1, 4, 8 weeks prior program entry week\n",
    "    pivot_wider(names_from = spend_week, values_from = avg_spend) %>% # Reshape the data to wide format so that each week's spending index is a column\n",
    "    `colnames<-`(c('county_fips_code', 'avg_spend_8', 'avg_spend_4', 'avg_spend_1')) %>% # Rename columns\n",
    "    mutate(cohort = 'cohort2') # Generate the cohort indicator\n",
    "\n",
    "# Append(Combine) the two DataFrames\n",
    "df_avg_spend <- rbind(df_cohort1_avg_spend, df_cohort2_avg_spend)\n",
    "\n",
    "# See the top records of the DataFrame\n",
    "head(df_avg_spend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Combine Datasets**\n",
    "\n",
    "Now we can join the claims data and the consumer spending data on county FIPS code (`county_fips_code`) and the cohort indicator (`cohort`). Note that we use `inner_join` to keep observations that are in both datasets, i.e., claimants who are living in the REDACTED counties that are included in the consumer spending data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inner join the claims data with the average spending data on county FIPS code and the cohort indicator\n",
    "#Inner join keeps the rows that are in both df_claimants and df_avg_spend\n",
    "df_claimants <- df_claimants %>%\n",
    "    inner_join(df_avg_spend, by = c('county_fips_code','cohort'))\n",
    "\n",
    "#See top records of the DataFrame\n",
    "head(df_claimants)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Create the Label**\n",
    "\n",
    "In this section, we construct the label (i.e., the outcome variable, the dependent variable, or Y). The value of the label is usually binary, i.e., 0 or 1.[<sup>4</sup>](#fn4) <a id = \"12\"> </a> Since we are interested in predicting claimants who will stay for 13 weeks or longer in the UI program, we define the label the following way:\n",
    "\n",
    "- **label = 1, if the claimant stayed 13 weeks or longer**\n",
    "- **label = 0, if the claimant left before the 13th week**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the label, we need to generate an indicator `week_number` first. It represents the number of weeks after program entry (similar to what we did in the [cohort analysis notebook](./2.Data_Exploration_Cohort_Analysis.ipynb)). Note that a small number of claimants' benefit year starting weeks are not consistent over time. We assume it's an data error and adjust these people's `byr_start_week` to their earliest starting weeks. This adjustment only applies to the cohorts we are looking at, because the changes we observe are mostly one week. Also, we only have one year of data. It's very unlikely that a person started two benefit years. If you are looking at multiple years of data and longer term outcomes, you need to investigate whether the changes are due to data error or multiple program entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define **stayers** as claimants who consistenly showed in the UI program over time. If a claimant left the UI program for one week, we assume he/she would not return and define this person as an **exiter**. Based on this definition, we only need a claimant's records during the first 13 weeks after program entry to create the label. Then, we count each claimant's number of records during the first 13 weeks. If the number of records is greater than 13, it implies that the person has stayed in the program for at least 13 weeks and his/her label is 1. If a person has less than 13 records, he/she must have left the program during the initial 13 weeks and his/her label is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few people's benefit year starting date changed, adjust them to the earliest date\n",
    "df_claimants <- df_claimants %>%\n",
    "    group_by (ssn_id, cohort) %>%\n",
    "    mutate(byr_start_week = min(byr_start_week)) %>% # For each person in each cohort, adjust the 'byr_start_week' to the earliest date\n",
    "    ungroup() #return data to non-grouped form. If we don't include this code, you may get errors in the later analysis\n",
    "\n",
    "# Create week number field and keep the first 13 weeks of records for each person\n",
    "df_claimants <- df_claimants %>% \n",
    "    mutate(week_number = as.integer(difftime(week_end_date, byr_start_week, units = \"weeks\")) + 1) %>%\n",
    "    filter(week_number <= 13) # Only keep each person's first 13 records\n",
    "\n",
    "#Create the label\n",
    "df_label <- df_claimants %>% \n",
    "    group_by(ssn_id, cohort) %>%\n",
    "    summarize(stay_weeks = n()) %>% #count each person's number of records\n",
    "    mutate(label = case_when(stay_weeks >= 13 ~ 1, TRUE ~ 0)) %>% #create the label, if stayed 13 weeks or more, then label=1, otherwise, label=0\n",
    "    ungroup() #return data to non-grouped form. If we don't include this code, you may get errors in the later analysis\n",
    "\n",
    "#Check the top rows of df_label\n",
    "head(df_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below calculates the percentages of faster exiters and slow exiters in each cohort. This helps us understand the distribution of our label. Depending on the context of your project, you may need to adjust your label definitions and/or research questions. For example, if only 1% of claimants stayed for 13 weeks or longer, we may want to change our goal and use a stricter definition, such as whether a claimant left the program within 6 weeks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand what percent of claimants in each cohort left before the 13th week\n",
    "\n",
    "#Count number of people by cohort and label\n",
    "df_label_freq <- df_label %>% group_by(cohort,label) %>% summarize(freq = n())\n",
    "\n",
    "#Count number of claimants in each cohort\n",
    "df_cohort_pop <- df_label %>% group_by(cohort) %>% summarize(cohort_pop = n_distinct(ssn_id))\n",
    "\n",
    "#Left join the two DataFrame and Calculate the percentages of fast exiters and slow exiters in each cohort\n",
    "df_label_freq <- df_label_freq %>%\n",
    "    left_join(df_cohort_pop, by = 'cohort') %>%\n",
    "    mutate(percent = round((freq/cohort_pop),3))\n",
    "\n",
    "#Look at the statistics\n",
    "df_label_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.Create the Features**\n",
    "\n",
    "In this section, we clean up the rest of the variables and create the features (i.e., predictor variables, independent variables, or X). The list below shows the variables that are available to us in this project. There are two types of features: categorical features and numeric features. Later, we will transform them into the format that we can use in a machine learning model. But first, we need to clean up the data, generate a few variables, such as age at program entry, and regroup categorical variables, such as converting 6-digit NAICS code to 2-digit NAICS code.\n",
    "\n",
    "- **Categorical features**: gender, race, ethnicity, disability, education, naics_code, occupation_code \n",
    "- **Numeric features**: birth_date(age), total_pay(weekly average), wages_2019, county-level average consumer spending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clean Up Data and Features**\n",
    "\n",
    "Our current claims data is in long format, i.e., each record represents a certified claim during a specific week and each claimant could have more than one record. However, **in the data we need for the machine learning model, each row should represent a claimant in a specific cohort.** Technically, a claimant can show in both cohorts, but since our first cohort entered only 13 weeks prior to the second cohort, this case doesn't apply to our data. Therefore, in this model, you can also interpret each row as a claimant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our features are time-invariant, except for the total pay. It may change from week to week due to part-time employment, start and end of the Federal Pandemic Unemployment Compensation (FPUC), etc. Since we only need one record for each claimant, we need to aggregate the data on `ssn_id` and `cohort` and calculate each claimant's average weekly total pay first. \n",
    "\n",
    "Moreover, in the administrative data, we often find that some people's time-invariant attributes change over time, such as gender and race. It could be data errors and/or corrections of wrong attributes in the most recent records. In our data, we also find that a few claimants' characteristics are different across records. In this case, we sort the data ascendingly by using `ssn_id` and `week_end_date` and for each claimant, we use his/her first record in the data. Some people's characteristics may have been corrected in the more recent records. However, remember that at the time of the prediction (7/5/2020), we don't know the testing cohort's corrected data yet and we need to avoid using data from the \"future\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we need to calculate average weekly total pay\n",
    "df_claimants <- df_claimants %>%\n",
    "    group_by(ssn_id, cohort) %>%\n",
    "    mutate(avg_total_pay = mean(total_pay))\n",
    "\n",
    "#Keep each person's first record, since we only need one set of features for each person\n",
    "df_feature <- df_claimants %>% \n",
    "    arrange(ssn_id, week_end_date) %>% #Sort the data ascendingly based on ssn_id and week_end_date\n",
    "    group_by(ssn_id, cohort) %>%\n",
    "    mutate(record_id = row_number()) %>% #create record ID for each person\n",
    "    filter(record_id == 1)  %>% #Keep each person's first record \n",
    "    ungroup() #return data to non-grouped form. If we don't include this code, you may get errors in the later analysis\n",
    "\n",
    "# See top records of df_feature\n",
    "head(df_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our data is in the format we want. Next, we generate age at program entry and regroup categorical variables. To calculate age at program entry, we use the `difftime` function to get the number of days between a person's date of birth and date of UI program entry and then divide it by 365.25. A small number of people's ages are too young. We bottom code their ages to REDACTED. For the other categorical variables, we regroup them in the same way as we suggested in the two data exploration notebooks. **If a variable has missing values, we don't suggest to drop them. We usually create an \"Unknown\" category for these variables.**\n",
    "\n",
    "- **Race**: white, African American, other race, race unknown\n",
    "- **Education Level**: less than 12 years, high school graduate, Associate's degree, Bachelor's degree, Master degree or higher, education unknown\n",
    "- **Occupation**: use 2-digit occupation codes and combine small categories if needed\n",
    "- **Industry**: use 2-digit NAICS codes and combine small industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate age at program entry and bottom code outliers\n",
    "df_feature <- df_feature %>%\n",
    "    mutate(age = as.integer(difftime(byr_start_week, birth_date, units='days')/365.25)) %>%\n",
    "    mutate(age = case_when(age<REDACTED ~ as.integer(REDACTED), TRUE~age)) # A small number of people are too young. We bottom code their ages\n",
    "\n",
    "# Recode gender, race, ethnicity, disability, and education\n",
    "df_feature <- df_feature %>%\n",
    "    mutate(gender = case_when(gender == 1 ~ 'Male', gender == 2 ~ 'Female', TRUE ~ 'Unknown'),\n",
    "           race = case_when(race == 1 ~ 'White', race == 2 ~ 'African_American', race %in% c(3, 4, 6) ~ 'Other_race', TRUE ~ 'Unknown'),\n",
    "           ethnicity = case_when(ethnicity == 1 ~ 'Hispanic', ethnicity == 2 ~ 'Not_Hispanic', TRUE ~ 'Unknown'),\n",
    "           disability = case_when(disability == 1 ~'Disabled', disability == 2 ~ 'Not_Disabled', TRUE ~ 'Unknown'),\n",
    "           education = case_when (education >= 1 & education <= 13 ~ \"Less_than_HS\",\n",
    "                                    education >= 14 & education <= 18 ~ \"HS_graduate_or_some_college\",\n",
    "                                    education >= 19 & education <= 20 ~ \"Associate\",\n",
    "                                    education >= 21 & education <= 22 ~ \"Bachelor\",\n",
    "                                    education >= 23 ~ \"Master_or_higher\",\n",
    "                                    TRUE ~ \"Other\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine NAICS major codes based on grouping used for UI dashboard \n",
    "# Import NAICS code crosswalk\n",
    "naics_groups <- read_csv('P:\\\\tr-dol-eta\\\\ETA Class Notebooks\\\\xwalks\\\\naics_groups.csv', col_types = \"ccc\")    \n",
    "\n",
    "# Convert 6-digit NAICS codes to 2-digit NAICS codes by keeping only the first two characters\n",
    "df_feature <- df_feature %>% mutate(naics_maj_code = substr(naics_code,1,2))\n",
    "\n",
    "# Join NAICS groupings to claimant dataset\n",
    "df_feature <- df_feature %>% \n",
    "    left_join(naics_groups, by = 'naics_maj_code') %>%\n",
    "    mutate(naics_maj_code_rv = case_when(naics_maj_code %in% c('REDACTED') ~ 'REDACTED', TRUE ~ naics_maj_code_rv)) \n",
    "\n",
    "# Check the top records of df_feature\n",
    "head(df_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have created all the variables we need. Let's remove the columns we no longer need so that it's easier to manage the data and we don't accidentally use them in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keep the columns we need\n",
    "df_feature <- df_feature %>% \n",
    "    select(ssn_id, cohort, byr_start_week, # Variables to identify person and cohort\n",
    "           gender, race, ethnicity, disability, education, naics_maj_code_rv, occupation_code, # Categorical Variables\n",
    "           wages_2019, avg_spend_1, avg_spend_4, avg_spend_8, avg_total_pay, age) #Numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Check Missing Values**\n",
    "\n",
    "Before we run a machine learning model, we want to make sure that there is no missing value in our data. Previously, we have recoded missing categorical variables into \"unknown\" categories. Let's check if our data has other missing values. The code below returns the number of missing values for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check which columns have missing values\n",
    "sapply(df_feature, function(x) sum(is.na(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some claimants' 2019 earnings are missing. There are various reasons for missing 2019 earnings. For example, some people may have only worked in 2020 but have earned enough incomes to be qualified for UI benefits. Depending on your assumptions about the reasons of missing earnings, you may impute the earnings in different ways. Here, we replace missing 2019 earnings with the average earnings of claimants with the same education level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in missing values\n",
    "#Replace missing 2019 earnings with the education level mean earnings\n",
    "df_feature <- df_feature %>%\n",
    "    group_by(education) %>%\n",
    "    mutate(wages_2019 = replace(wages_2019, is.na(wages_2019), mean(wages_2019, na.rm = TRUE))) %>%\n",
    "    ungroup()  #return data to non-grouped form. If we don't include this code, you may get errors in the later analysis\n",
    "\n",
    "#Check missing values again\n",
    "sapply(df_feature, function(x) sum(is.na(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Numeric Features**\n",
    "\n",
    "Next, we need to convert the features into formats that can be used in ML models. **Specifically, we use the `scale()` function to standardize all of our numeric features.** That way, the importance of variables of different magnitudes/units can be assessed on the same scale. This also enables our models to work more efficiently than when we don't scale numeric variables. For example, `wages_2019` has values such as 1,000 or 10,000, but `age` is less than 100. We usually consider 10 years as a big age difference, but $100 as a very small annual income difference. However, in a distance-based ML algorithm, these differences are treated as numbers and a difference of 100 is more significant than a difference of 10. It's very likely that annual income will be used as a more important predictor than age simplily because income has a larger magnitude than age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scale()` function substracts a variable by its mean and then divides it by its standard deviation. The scaled variables's means are 0 and standard deviations are 1. **Note that you need to scale the training set's and the testing set's data separately**, because a variable's mean and standard deviation may be different in the training set than in the testing set. In the code below, we group the data by `cohort` first and then scale the numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the numeric variables by cohort\n",
    "df_feature <- df_feature %>%\n",
    "    group_by(cohort) %>%\n",
    "    mutate(age_scaled = scale(age), avg_total_pay_scaled = scale(avg_total_pay), \n",
    "           avg_spend_1_scaled = scale(avg_spend_1), avg_spend_4_scaled = scale(avg_spend_4),\n",
    "           avg_spend_8_scaled = scale(avg_spend_8), wages_2019_scaled = scale(wages_2019)) %>%\n",
    "    select(-c(age, avg_total_pay, avg_spend_1, avg_spend_4, avg_spend_8,wages_2019)) %>% # Remove columns we don't need\n",
    "    ungroup()  #return data to non-grouped form. If we don't include this code, you may get errors in the later analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Categorical Features**\n",
    "We also use the `factor` function to turn all the categorical features into factor type. ML model functions will implement them as dummy variables. Note that we will write the data to a csv file and read it in the second ML notebook. During this process, factor type columns will be converted back to character type. So, **we will only use `gender` as an example to show you how to use the `factor` function here. We will convert all the categorical features into factor type at the beginning of the next notebook.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gender as an example\n",
    "# Convert character type into factor type; numeric variables can be converted into factor type as well\n",
    "df_feature$gender <- factor(df_feature$gender)\n",
    "\n",
    "#Check the types of ssn_id, gender, race. They should have <int>, <fct>, <chr> types, respectively.\n",
    "head(df_feature %>%  select(ssn_id, gender, race))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.Combine DataFrames**\n",
    "\n",
    "Finally, we combine the label DataFrame we created in section 4, `df_label`, and the feature DataFrame we created in section 5, `df_feature`. We export the final DataFrame `df_ml` to a csv file and will use it in the second ML notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the label DataFrame (df_label) \n",
    "# and the feature DataFrame with dummy variables and scaled numeric variables (df_feature_trans)\n",
    "df_ml <- df_label[,c('ssn_id','cohort','label')] %>%\n",
    "    left_join(df_feature, on = c('ssn_id','cohort'))\n",
    "\n",
    "# See top records of the final DataFrame\n",
    "head(df_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> Note that you need to create a \"Data\" folder in your \"ETA Training\" folder first. Then change the directory in write.csv() statements below. Replace \". .\" with your username.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data to a csv file. We will use it in the next notebook\n",
    "write.csv(df_ml, \"U:\\\\..\\\\ETA Training\\\\Data\\\\ml_data.csv\", row.names=F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Footnotes:**\n",
    "<span id=\"fn1\"> 1. The data is publicly available and can be downloaded  <a href='https://github.com/OpportunityInsights/EconomicTracker'>here</a>. </span>  \n",
    "\n",
    "[[Go back]](#9)\n",
    "\n",
    "<span id=\"fn2\"> 2. Chetty, R., Friedman, J. N., Hendren, N., Stepner, M., & The Opportunity Insights Team. (2020). <a href='https://opportunityinsights.org/wp-content/uploads/2020/05/tracker_paper.pdf'>The economic impacts of COVID-19: Evidence from a new public database built using private sector data</a> (No. w27431). National Bureau of Economic Research. </span>  \n",
    "\n",
    "[[Go back]](#10)\n",
    "\n",
    "<span id=\"fn3\"> 3. For more information about the data coverage, see: <a href='https://github.com/OpportunityInsights/EconomicTracker/blob/main/docs/oi_tracker_data_documentation.md'> Opportunity Insights Economic Tracker Data Documentation. </a> </span>  \n",
    "\n",
    "[[Go back]](#11)\n",
    "\n",
    "<span id=\"fn4\"> 4.<a href='https://github.com/dssg/hitchhikers-guide/blob/master/sources/curriculum/3_modeling_and_machine_learning/machine-learning/machine_learning_clean.ipynb'> Hitchhiker's Guide to Data Science for Social Good. </a> </span>  \n",
    "\n",
    "[[Go back]](#12)\n",
    "\n",
    "> Note that the above links don't work inside of the ADRF since you don't have internet access.\n",
    "\n",
    "> Click [Go back] to go back to where you were."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
